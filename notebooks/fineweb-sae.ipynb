{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import latentscope as ls\n",
    "import h5py\n",
    "# !pip install datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: show downloading sample from fineweb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = '/Users/enjalot/latent-scope-data/fineweb-edu-10k/saes/sae-001.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/enjalot/latent-scope-data/fineweb-edu-10k/saes/sae-001.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(f\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      3\u001b[0m     top_acts \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_acts\u001b[39m\u001b[38;5;124m\"\u001b[39m)[:]\n",
      "File \u001b[0;32m~/code/latent-scope/venv/lib/python3.11/site-packages/h5py/_hl/files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/code/latent-scope/venv/lib/python3.11/site-packages/h5py/_hl/files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = '/Users/enjalot/latent-scope-data/fineweb-edu-10k/saes/sae-001.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"/Users/enjalot/latent-scope-data/fineweb-edu-10k/saes/sae-001.h5\", \"r\") as f:\n",
    "    print(f.keys())\n",
    "    top_acts = f.get(\"top_acts\")[:]\n",
    "    top_indices = f.get(\"top_indices\")[:]\n",
    "    print(top_acts.shape)\n",
    "    print(top_indices.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_acts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtop_acts\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'top_acts' is not defined"
     ]
    }
   ],
   "source": [
    "top_acts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8944,  7583, 16225, 14943, 11176, 23917,  4919,  4941,  4000,\n",
       "       10367,  5983,  7627,  9398, 16267, 11963, 16708,  7518, 18204,\n",
       "        9535,  2724, 18167,  2228, 19213, 18010, 22989,  5432, 10935,\n",
       "       22337,  4004, 11119,  7774, 17695, 10567, 21551, 23250,  1460,\n",
       "       22268,  9566, 18271, 21744, 21539, 10463, 20740,  5340, 10539,\n",
       "        9217, 15487,  7070, 22817, 17483, 24236, 18312, 20104, 12048,\n",
       "        3322,    38,  4430, 13258,  3523,  1725, 20912, 15362,  8929,\n",
       "         519])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304d92ef429f45b7a373eb404c5c796f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "umap-top10.pkl:   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbef00423d34c35bb222f6a8f62ffac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "umap/64_32/umap-top10-metadata.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# experiment with loading a saved UMAP model for the SAE top activations that we can use to map any dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Download the UMAP model from HuggingFace Hub\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"enjalot/sae-nomic-text-v1.5-FineWeb-edu-100BT\",\n",
    "    filename=\"umap/64_32/umap-top10.pkl\"\n",
    ")\n",
    "# Download the metadata JSON file\n",
    "metadata_path = hf_hub_download(\n",
    "    repo_id=\"enjalot/sae-nomic-text-v1.5-FineWeb-edu-100BT\",\n",
    "    filename=\"umap/64_32/umap-top10-metadata.json\"\n",
    ")\n",
    "\n",
    "\n",
    "# Load the UMAP model\n",
    "with open(model_path, 'rb') as f:\n",
    "    umap_model = pickle.load(f)\n",
    "\n",
    "\n",
    "# Load the metadata\n",
    "with open(metadata_path, 'r') as f:\n",
    "    umap_metadata = json.load(f)\n",
    "\n",
    "print(\"UMAP model loaded:\", umap_model)\n",
    "print(\"metadata\", umap_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load some embeddings and fit this\n",
    "xy = umap_model.transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_values = np.array([umap_metadata['bounds'][\"x_min\"], umap_metadata['bounds'][\"y_min\"]])\n",
    "max_values = np.array([umap_metadata['bounds'][\"x_max\"], umap_metadata['bounds'][\"y_max\"]])\n",
    "normalized_points = (xy - min_values) / (max_values - min_values)\n",
    "normalized_points = 2 * normalized_points - 1\n",
    "# you can now get the x,y coordinates that are scaled to the original umap x,y\n",
    "# this means any dataset projected in this way will overlap in the exact same space as the original SAE top activations\n",
    "\n",
    "# results_df[\"x\"] = normalized_points[:, 0]\n",
    "# results_df[\"y\"] = normalized_points[:, 1]\n",
    "# results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# Load the nomic text embed model\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "model_name = \"nomic-ai/nomic-embed-text-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "def mean_pooling(token_embeddings, attention_mask):\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "def get_hidden_states(text, model, tokenizer):\n",
    "    # Tokenize the text and move to GPU if available\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get hidden states\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    # Get the last hidden state\n",
    "    embedding = mean_pooling(outputs[0], inputs[\"attention_mask\"])\n",
    "    return { \"hidden_states\": outputs[0].cpu().numpy(), \"embedding\": embedding.cpu().numpy() }\n",
    "\n",
    "# Example usage:\n",
    "text = [\"Where do dads keep their dad jokes? In a dad-a-base\"]\n",
    "hs = get_hidden_states(text, model, tokenizer)\n",
    "pertoken = hs[\"hidden_states\"]\n",
    "embedding = hs[\"embedding\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 18, 768)\n",
      "(1, 768)\n"
     ]
    }
   ],
   "source": [
    "print(pertoken.shape)\n",
    "print(embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3995b2378e9a4693bd602cf0919c6496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    }
   ],
   "source": [
    "from latentsae.sae import Sae\n",
    "\n",
    "# Use same model_id and k_expansion as in sae.py\n",
    "model_id = \"enjalot/sae-nomic-text-v1.5-FineWeb-edu-100BT\"\n",
    "k_expansion = \"64_32\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "\n",
    "# Load SAE model\n",
    "sae_model = Sae.load_from_hub(model_id, k_expansion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding embeddings...\n",
      "\n",
      "Encoding per-token hidden states...\n",
      "\n",
      "Embedding features:\n",
      "Top activations shape: torch.Size([1, 64])\n",
      "Top indices shape: torch.Size([1, 64])\n",
      "Sample top indices: tensor([ 6328,  6939, 24120,  3945,  5750,  6073, 13056,  9757, 10978, 22978])\n"
     ]
    }
   ],
   "source": [
    "# Convert numpy arrays to torch tensors and move to device\n",
    "embedding_tensor = torch.from_numpy(embedding).float().to(device)\n",
    "pertoken_tensor = torch.from_numpy(pertoken).float().to(device)\n",
    "\n",
    "# Reshape pertoken to 2D (combine batch and sequence dims)\n",
    "batch_size, seq_len, hidden_dim = pertoken_tensor.shape\n",
    "pertoken_2d = pertoken_tensor.reshape(-1, hidden_dim)\n",
    "\n",
    "print(\"Encoding embeddings...\")\n",
    "embedding_features = sae_model.encode(embedding_tensor)\n",
    "\n",
    "print(\"\\nEncoding per-token hidden states...\")\n",
    "pertoken_features = sae_model.encode(pertoken_2d)\n",
    "\n",
    "# Print shapes and sample activations\n",
    "print(\"\\nEmbedding features:\")\n",
    "print(\"Top activations shape:\", embedding_features.top_acts.shape)\n",
    "print(\"Top indices shape:\", embedding_features.top_indices.shape)\n",
    "# print(\"Sample top activations:\", embedding_features.top_acts[0][:5])\n",
    "print(\"Sample top indices:\", embedding_features.top_indices[0][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-token features sorted by activation:\n",
      "Shape: torch.Size([18, 64])\n",
      "Top 10 most activated indices for first token: [ 6939  6328  3945 13923 24120 19936 22978 13056 16387  5587]\n"
     ]
    }
   ],
   "source": [
    "# Get the top activations and indices from pertoken features\n",
    "pertoken_acts = pertoken_features.top_acts\n",
    "pertoken_inds = pertoken_features.top_indices\n",
    "\n",
    "# Sort indices by activation values\n",
    "sorted_acts, sorted_idx = torch.sort(pertoken_acts, dim=1, descending=True)\n",
    "\n",
    "# Use the sorted indices to get the corresponding feature indices\n",
    "sorted_feature_inds = torch.gather(pertoken_inds, 1, sorted_idx)\n",
    "\n",
    "print(\"\\nPer-token features sorted by activation:\")\n",
    "print(\"Shape:\", sorted_feature_inds.shape)\n",
    "print(\"Top 10 most activated indices for first token:\", sorted_feature_inds[0,:10].cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -e .\n",
    "from latentsae.widgets import TopK, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sae_features = pd.read_parquet(\"https://enjalot.github.io/latent-taxonomy/models/NOMIC_FWEDU_25k/features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = sae_features.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maxk(directions, k=32):\n",
    "    # Flatten the top_acts and top_indices to find the global top k\n",
    "    flat_acts = directions.top_acts.flatten()\n",
    "    flat_indices = directions.top_indices.flatten()\n",
    "\n",
    "    # Create a dictionary to keep track of the highest activation for each index\n",
    "    max_acts = {}\n",
    "    \n",
    "    for act, idx in zip(flat_acts, flat_indices):\n",
    "        if idx.item() not in max_acts or act.item() > max_acts[idx.item()]:\n",
    "            max_acts[idx.item()] = act.item()\n",
    "    \n",
    "    # Convert the dictionary back to tensors\n",
    "    flat_acts = torch.tensor(list(max_acts.values()))\n",
    "    flat_indices = torch.tensor(list(max_acts.keys()))\n",
    "    \n",
    "    # Get the top k activations and their corresponding indices\n",
    "    topk_acts, topk_indices = torch.topk(flat_acts, k)\n",
    "    \n",
    "    # Map the flat indices back to the original indices\n",
    "    original_indices = flat_indices[topk_indices]\n",
    "    \n",
    "    topk = {\n",
    "        \"top_acts\": topk_acts,\n",
    "        \"top_indices\": original_indices\n",
    "    }\n",
    "    return topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_direction(directions, idx):\n",
    "    return { \"top_indices\": directions.top_indices[idx], \"top_acts\": directions.top_acts[idx] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxk = get_maxk(pertoken_features, k=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>document.querySelectorAll('style#topk-widget-css').forEach(e => e.remove())</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style id='topk-widget-css'>\n",
       ".widget-topk-vis {\n",
       "  width: calc(100%);\n",
       "  height: 100%;\n",
       "  padding: 10px;\n",
       "  color: #111;\n",
       "}\n",
       ".sampleActivationBar {\n",
       "  width: 95%; \n",
       "  border: 1px solid lightgray;\n",
       "  height: 14px;\n",
       "  position: relative;\n",
       "  margin: 1px;\n",
       "  overflow: hidden;\n",
       "}\n",
       ".sampleActivationBarForeground {\n",
       "  height: 12px;\n",
       "  opacity: 0.7;\n",
       "}\n",
       ".sampleActivationBarForeground:hover {\n",
       "  opacity: 1;\n",
       "}\n",
       ".sampleActivationBarLabel {\n",
       "  font-size: 10px;\n",
       "  position: absolute;\n",
       "  top: -2px;\n",
       "  left: 4px;\n",
       "  display: flex;\n",
       "  flex-direction: row;\n",
       "  align-items: center;\n",
       "  justify-content: space-between;\n",
       "  pointer-events: none;\n",
       "  width: 95%;\n",
       "  overflow: hidden;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c53ba9e46d422582adb5b07628d406",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "TopK(data={'top_acts': [12.723018646240234, 11.715324401855469, 8.681981086730957, 8.520795822143555, 8.355340…"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TopK(data=maxk, n=30, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>document.querySelectorAll('style#topk-widget-css').forEach(e => e.remove())</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style id='topk-widget-css'>\n",
       ".widget-topk-vis {\n",
       "  width: calc(100%);\n",
       "  height: 100%;\n",
       "  padding: 10px;\n",
       "  color: #111;\n",
       "}\n",
       ".sampleActivationBar {\n",
       "  width: 95%; \n",
       "  border: 1px solid lightgray;\n",
       "  height: 14px;\n",
       "  position: relative;\n",
       "  margin: 1px;\n",
       "  overflow: hidden;\n",
       "}\n",
       ".sampleActivationBarForeground {\n",
       "  height: 12px;\n",
       "  opacity: 0.7;\n",
       "}\n",
       ".sampleActivationBarForeground:hover {\n",
       "  opacity: 1;\n",
       "}\n",
       ".sampleActivationBarLabel {\n",
       "  font-size: 10px;\n",
       "  position: absolute;\n",
       "  top: -2px;\n",
       "  left: 4px;\n",
       "  display: flex;\n",
       "  flex-direction: row;\n",
       "  align-items: center;\n",
       "  justify-content: space-between;\n",
       "  pointer-events: none;\n",
       "  width: 95%;\n",
       "  overflow: hidden;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19049a30a7c44879bd6c1b4e91121789",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "TopK(data={'top_indices': [6328, 6939, 24120, 3945, 5750, 6073, 13056, 9757, 10978, 22978, 20602, 13923, 22022…"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TopK(data=get_token_direction(embedding_features, 0), n=10, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
